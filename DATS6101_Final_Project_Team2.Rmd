---
title: "Lifecycle and mental health"
author: "Ishani Makwana, Henry Hirsch and Ei Tanaka"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# Some of common RMD options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
```
Loading all the necessary libraries which are needed for this project.
A brief explanation of each of the libraries used in this R Markdown script:

readr: This is a package used for reading and writing structured data files, such as CSV files. It provides fast and efficient functions for reading in data, including read_csv.

ggplot2: This is a package used for creating data visualizations. It provides a highly customizable grammar of graphics for creating plots with a wide range of styles and features.

tidyr: This is a package used for data wrangling, specifically for reshaping and tidying data. It provides functions like gather and spread for transforming data between "wide" and "long" formats.

corrplot: This is a package used for creating correlation matrices and visualizing correlations between variables. It provides several options for customizing the appearance of correlation plots.

ezids: This is a package used for generating and manipulating EZID identifiers, which are persistent, globally unique identifiers for digital objects. It provides functions for minting, modifying, and resolving EZID identifiers.

car: This is a package used for conducting regression analyses and other statistical tests. It provides several functions for model diagnostics, including outlierTest and leveragePlots.

rpart: This is a package used for creating decision trees. It provides functions for building and evaluating decision tree models.

rpart.plot: This is a package used for visualizing decision trees created using rpart. It provides several options for customizing the appearance of tree plots.

rattle: This is a package used for data mining and machine learning. It provides a graphical interface for building and evaluating models, as well as several functions for data preprocessing and visualization.

tree: used for creating classification and regression trees. It provides functions for building decision trees using various algorithms, including the Classification And Regression Tree (CART) algorithm.
 
```{r}
# Import Libraries
rm(list=ls())
library(readr)
library(ggplot2)
library(tidyr)
library(corrplot)
library(ezids)
library(car)
library(rpart)
library(rpart.plot)
library(rattle)
library(tree)
```
As for the dataset, the script reads in a CSV file from a URL using read_csv from the readr package. The URL points to a CSV file containing socio-economic and health data for various geographic regions. After reading in the data, the first column of the data frame is removed using [-1], presumably because it contains redundant or unnecessary information.
```{r init}
url <- 'https://raw.githubusercontent.com/eitanaka/DATS6101_Final_Project_Team2/main/data_set/geo_socio_health_df.csv'
master_df<- read_csv(url)
master_df <- master_df[,-1]
```

## Abstract
Need edited

This research project aims to investigate the relationship between mental health and various socio-economic and lifestyle factors, as well as the impact of COVID-19, in the United States. The primary objective is to determine whether these factors significantly influence mental health in the US, using 11 different datasets analyzed at the census tract level. The introduction provides a background and rationale for the research question and the datasets used.

The project uses various statistical techniques, including correlation analysis, regression analysis, and decision tree analysis, to identify the most significant predictors of mental health outcomes. In addition, the study contributes to the literature on the impact of socio-economic and lifestyle factors on mental health.

Overall, this research project offers a comprehensive analysis of the relationship between mental health and various factors, providing insights into potential interventions to improve mental health outcomes.

## Introduction
Our previous project 1 focused on the relationship between health risk behaviors and health outcomes and status, using a range of datasets. In this research project, we shift our focus to investigating the influence of lifestyle, socio-economic factors, and COVID-19 on mental health in the United States.

We aim to determine which factors have the most significant impact on mental health and well-being in the US during 2020. To do so, we will analyze 11 different datasets at the census tract level, utilizing various statistical techniques such as correlation analysis, regression analysis, and decision tree analysis.

The results of this study have important implications for improving mental health outcomes and shaping public health policies and interventions. By identifying the factors that most significantly impact mental health, we can develop targeted interventions and programs to address these issues.

Overall, this research project builds upon our previous work and adds to the growing body of research on mental health and well-being in the United States. By exploring the relationships between lifestyle, socio-economic factors, and COVID-19 with mental health outcomes, we hope to contribute to a greater understanding of these issues and help improve mental health outcomes in the US.

### Data sets
Need edited (Describe about 4 data set and how we create)

Our research project utilizes 10 different datasets to investigate the influence of lifestyle, socio-economic factors, and COVID-19 on mental health outcomes in the United States. One of these datasets is carried over from our previous Data Science project (Project 1), while the other were collected specifically for this study.

1) CDC_PLACE dataset, which measures health outcomes, prevention, health risk behaviors, and health status. This dataset contains 13 health outcomes, 9 prevention measures, 4 health risk behaviors, and 3 health status measures.The original dataset was launched by the Centers for Disease Control and Prevention (CDC)- In 2020, the dataset provided small area estimates  for counties, places, census tracts, and ZIP Code Tabulation Areas across the entire United States.  Each measure has a comprehensive definition that includes the background, significance, limitations of the indicator, data source, and limitations of the data resources.

2) The American Community Survey 2020 is another dataset used in our study. This ongoing survey provides detailed information on population and housing in the US on a yearly basis.The ACS  helps local officials, community leaders, and businesses understand the changes taking place in their communities.    Through the ACS, we know more about jobs and occupations, educational attainment, veterans, whether people own or rent their homes, and other topics. 


3)Planning Database 2020, which contains operational, demographic, and socio-economic statistics from the 2010 Census at both the tract and block group levels.CEII data also includes annualized monthly estimates of county-level value added for more than 100 industries. Counties with economic activities dominated by industries experiencing rising unemployment can expect larger direct impacts to their local economies, particularly if the industries account for a large portion of the economic output of that county.

4) County Economic Impact Index 2020, which estimates the change in overall county-level economic activity during the COVID-19 pandemic relative to 2020. These four datasets, along with the six others, were analyzed to investigate the relationship between mental health outcomes and various socio-economic and lifestyle factors, as well as the impact of COVID-19.

### Initial SMART Question
Need edited
SQ1: What factors do we observed highly associated with depression and poor mental health rates?

SQ 2: Using those factors, how accurately can we infer depression and poor mental health rates in a given tract?

### Data Cleaning and preparing
Data cleaning and preparation is a crucial step in any data analysis project. In this step, we make sure that the data is in a format that is suitable for analysis, and that any inconsistencies, errors, or missing values are corrected or removed.

In our project, we started by merging the four data sources by countyFIPs and Tract level Geographical ID. This allowed us to combine information from different sources based on their common identifiers. We also renamed variables to make them more meaningful and easier to understand.

After merging and renaming variables, we removed all rows including null values. This was important because null values can interfere with our analysis and lead to inaccurate conclusions. We also got rid of all outliers to reduce the influence of extreme values on our analysis.

Once we completed these cleaning steps, we had a data frame with 12,444 observations of 57 variables. This data frame was ready for further analysis and exploration.


## EDA
Need edited

Exploratory Data Analysis (EDA) is a crucial step in any data analysis project, and in this project, we performed EDA to gain insights into the relationships between various socio-economic, lifestyle, and health factors and mental health outcomes in the United States.

After cleaning the data, we started by manipulating and visualizing the data. We renamed some columns in the dataframe to make them more readable and easier to work with. We also created two histograms using the ggplot2 library to visualize the distributions of two key variables. The first histogram, dep_hist, shows the distribution of tract-level depression rates, and the second histogram, mhlth_hist, shows the distribution of tract-level poor mental health rates.

Additionally, we performed various other visualization tasks such as scatter plots, box plots, and correlation matrices to explore the relationships between different variables. We also calculated descriptive statistics such as mean, median, and standard deviation to summarize the data and gain further insights.

Overall, EDA helped us understand the data and identify patterns and relationships that could be explored further through statistical analysis and modeling.
```{r}
# To rename some columns in a data frame to make them more readable and easier to work with.This is done by using the "colnames" function to first select the column names that match the original names, and then assigning new names.
colnames(master_df)[colnames(master_df) == "MT_Never Married"] <- "mt.nev.mar"
colnames(master_df)[colnames(master_df) == "MT_Now married"] <- "mt.now.mar"
colnames(master_df)[colnames(master_df) == "Total Population"] <- "tot.pop"
colnames(master_df)[colnames(master_df) == "EA_Less than high school graduate"] <- "ea.less.hs.deg"
colnames(master_df)[colnames(master_df) == "EA_High school graduate"] <- "ea.hs.deg"
colnames(master_df)[colnames(master_df) == "EA_college or associate's degree"] <- "ea.col.ass.deg"
colnames(master_df)[colnames(master_df) == "EA_Bachelor's degree"] <- "ea.ba.deg"
colnames(master_df)[colnames(master_df) == "EA_Graduate or professional degree"] <- "ea.grad.prof.deg"
```

```{r}
# Create a new data only including numerical variable
numeric_vars <- sapply(master_df, is.numeric)
num_df <- master_df[, numeric_vars]
```

#### Basic Information
```{r}
head(num_df)
str(num_df)
```

#### Handle Null Values
```{r}
colSums(is.na(num_df))
num_df <- na.omit(num_df)
colSums(is.na(num_df))
```
The output will be a numeric vector with the number of missing values in each column. If all values are 0, then there are no missing values in the selected columns.

#### Handle Outlier
```{r}
colSums(num_df == 0.0)

new_num_df <- outlierKD2(num_df, num_df[[1]], rm=TRUE)
new_num_df <- new_num_df[,1:(ncol(new_num_df)-2)]

# loop through all columns
for (col_name in colnames(num_df)[2:ncol(num_df)]) {
  # remove outliers
  new_num_df <- outlierKD2(new_num_df, new_num_df[[col_name]], rm=TRUE)
  new_num_df <- na.omit(new_num_df)
  new_num_df <- new_num_df[,1:(ncol(new_num_df)-2)]
}
new_num_df <- new_num_df[,1:(ncol(new_num_df)-1)]
```

### Boxplots for checking outliers
```{r}
# with outliers
par(mfrow=c(8, 8))
par(mar=c(1, 1, 1, 1))
for (i in 1:ncol(num_df)) {
  boxplot(num_df[[i]], main=names(num_df)[i])
}

# without outliers
par(mfrow=c(8, 8))
par(mar=c(1, 1, 1, 1))
for (i in 1:ncol(new_num_df)) {
  boxplot(new_num_df[[i]], main=names(new_num_df)[i])
}
```
### Scatter plot for checking outliers (Depression)
```{r}
# Scatter plots with fit line for Depression and other dependent variables
for (col in names(new_num_df)) {
  if (col != "DEPRESSION") {
    plot_data <- data.frame(x = new_num_df$DEPRESSION, y = new_num_df[[col]])
    plot <- ggplot(plot_data, aes(x = x, y = y)) + 
      geom_point() +
      geom_smooth(method = "lm", se = FALSE) +
      xlab("Depression") +
      ylab(col)
    print(plot)
  }
}
```

### Scatter plot for checking outliers (MHLTH)
```{r}
# Scatter plots with fit line for mhlth and other dependent variables
for (col in names(new_num_df)) {
  if (col != "MHLTH") {
    plot_data <- data.frame(x = new_num_df$MHLTH, y = new_num_df[[col]])
    plot <- ggplot(plot_data, aes(x = x, y = y)) + 
      geom_point() +
      geom_smooth(method = "lm", se = FALSE) +
      xlab("mhlth") +
      ylab(col)
    print(plot)
  }
}
```

### Data Structure
Need edited
It can be useful to get a quick overall structure of the data frame before performing further analyses.
```{r}
str(new_num_df)
```

### Descriptive Statistics
Need edited
It can be useful to get a overview  of summary statistics of the data frame before performing further analyses.
```{r}
summary(new_num_df)
```

### Histogram for dependent variable
To Check normality of dependent variable
```{r}
# Histograms
# Two histograms: one for the distribution of tract-level depression rates and one for the distribution of tract-level poor mental health rates.
hist(new_num_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")

hist(new_num_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")

```
Looking at the histograms, it appears that both variables are approximately normally distributed, although the distribution of tract-level depression rates is skewed to the right, with a larger number of observations in the higher bins. The distribution of tract-level poor mental health rates also appears skewed to the right, but with a more gradual decrease in the number of observations in the higher bins compared to the depression rates histogram.This information is important when selecting statistical methods to analyze these variables, as many methods assume normality. Additionally, the histograms provide a visual representation of the distribution of the variables, which can help in identifying potential outliers or unusual patterns.

### QQ-Plot for dependent variable
To check normality of dependent variable
```{r}
# QQ plots for the distribution of tract-level depression rates and tract-level poor mental health rates.

qqnorm(new_num_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")
qqline(new_num_df$DEPRESSION)

qqnorm(new_num_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")
qqline(new_num_df$MHLTH)
```
A QQ plot compares the quantiles of a dataset to the quantiles of a theoretical distribution (in this case, the normal distribution). If the points on the QQ plot are close to a straight line, it indicates that the data are normally distributed.
In this case, the QQ plot for the depression rates appears to deviate from a straight line towards the tails, indicating that the distribution is not perfectly normal. The QQ plot for the poor mental health rates appears to deviate from a straight line even more than the depression rates, indicating that the distribution is even less normal.

We will create a new data frame called num_df that only contains the numeric columns of the original master_df, that can be analyzed using various statistical tools.

### Correlations Test
```{r}
# Create a correlation matrix
cor_matrix <- cor(new_num_df)

# Create two lists which have the names of variables highly correlated (more then 0.3 or less than -0.3)
high_dep_cor_list <- names(which(cor_matrix["DEPRESSION",] > 0.35 | cor_matrix["DEPRESSION",] < -0.35))
high_mhlth_cor_list <- names(which(cor_matrix["MHLTH",] > 0.5 | cor_matrix["MHLTH",] < -0.5))

high_dep_cor_list <- high_dep_cor_list[high_dep_cor_list != "MHLTH"]
high_mhlth_cor_list <- high_mhlth_cor_list[high_mhlth_cor_list != "DEPRESSION"]

length(high_dep_cor_list)  
length(high_mhlth_cor_list)
```

### Correlation Heatmap
```{r}
# Create two correlation matrixes from new_num_df in the above lists
high_dep_cor_mat <- cor(new_num_df[high_dep_cor_list])
high_mhlth_cor_mat <- cor(new_num_df[high_mhlth_cor_list])

high_dep_cor_mat
high_mhlth_cor_mat

# Plot above correlation matrix
corrplot(high_dep_cor_mat, type = "lower", outline.color = "white", 
           colors = c("#6D9EC1", "white", "#E46726"), legend.title = "Correlation", 
           ggtheme = theme_gray, title = "Highly Correlated Variables with DEPRESSION")

corrplot(high_mhlth_cor_mat, type = "lower", outline.color = "white", 
           colors = c("#6D9EC1", "white", "#E46726"), legend.title = "Correlation", 
           ggtheme = theme_gray, title = "Highly Correlated Variables with mhlth")

my_col <- colorRampPalette(c("red", "white", "blue"))(30)

corrplot(high_dep_cor_mat, method = "number", type = "lower", order = "hclust", diag = FALSE, tl.col = "black", tl.cex = 0.8, cl.cex = 0.8, addCoef.col = "black", col = my_col, main = "", mar = c(0,0,0,0))

corrplot(high_mhlth_cor_mat, method = "number", type = "lower", order = "hclust", diag = FALSE, tl.col = "black", tl.cex = 0.8, cl.cex = 0.8, addCoef.col = "black", col = my_col, main = "", mar = c(0,0,0,0))

```

#### VIF Test for Multicollinearity
```{r}
calculate_VIF <- function(data, target_col) {
  X <- data[, !colnames(data) %in% target_col]
  vif <- data.frame(
    Feature = colnames(X),
    VIF = apply(X, 2, function(x) vif(lm(x ~ ., data=X)))
  )
  return(vif)
}

depression_VIF <- calculate_VIF(new_num_df[high_dep_cor_list], "DEPRESSION")

mhlth_VIF <- calculate_VIF(new_num_df[high_mhlth_cor_list], "MHLTH")

depression_VIF
mhlth_VIF
```

### Feature Selection
Need edited
```{r}
dep_features_list <- high_dep_cor_list[high_dep_cor_list != "DEPRESSION"]
mhlth_features_list <- high_mhlth_cor_list[high_mhlth_cor_list != "MHLTH"]

dep_features_list
mhlth_features_list
```

## Model Building
Need edited

### Training and Test Sets
```{r}
library(caret)
set.seed(123)
fold <- floor(runif(nrow(new_num_df),1,11)) 
  new_num_df$fold <- fold
  
test.set <- new_num_df[new_num_df$fold == 1,] 
train.set <- new_num_df[new_num_df$fold != 1,] 

```

### Multiple Linear Regression

#### Model 1: With Highly correlated variables (Health Factors + Socioeconomic Factors)
```{r}
# multiple linear regression model for depression
depression_model_1 <- lm(DEPRESSION ~ OBESITY + PHLTH + ea.hs.deg + ea.grad.prof.deg + MI_Estimate + `CT_>60` + hhd.broad, data = train.set)

# multiple linear regression model for poor mental health
mhlth_model_1 <- lm(MHLTH ~ LPA + OBESITY + PHLTH + SLEEP + ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + poverty, data = train.set)

summary(depression_model_1)
summary(mhlth_model_1)

plot(depression_model_1)
plot(mhlth_model_1)
```

#### Model 2: With Highly correlated health related variables
```{r}
# multiple linear regression model for depression
depression_model_2 <- lm(DEPRESSION ~ OBESITY + PHLTH, data = train.set)

# multiple linear regression model for poor mental health
mhlth_model_2 <- lm(MHLTH ~ LPA + OBESITY + PHLTH + SLEEP, data = train.set)

summary(depression_model_2)
summary(mhlth_model_2)

plot(depression_model_2)
plot(mhlth_model_2)
```

#### Model 3: With Highly correlated socioeconomic related variables
```{r}
# multiple linear regression model for depression
depression_model_3 <- lm(DEPRESSION ~ ea.hs.deg + ea.grad.prof.deg + MI_Estimate + `CT_>60` + hhd.broad, data = train.set)

# multiple linear regression model for poor mental health
mhlth_model_3 <- lm(MHLTH ~ ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + poverty, data = train.set)

summary(depression_model_3)
summary(mhlth_model_3)

plot(depression_model_3)
plot(mhlth_model_3)
```

#### Model Comparison (ANOVA)
```{r}
# Perform two-way ANOVA test
depression_anova_model <- anova(depression_model_1, depression_model_2, depression_model_3)
mhlth_anova_model <- anova(mhlth_model_1, mhlth_model_2, mhlth_model_3)


xkabledply(depression_anova_model)
xkabledply(mhlth_anova_model)

health.socioecon.dep.anova <- anova(depression_model_2, depression_model_3)
health.socioecon.mhlth.anova <- anova(mhlth_model_2, mhlth_model_3)

xkabledply(health.socioecon.dep.anova)
xkabledply(health.socioecon.mhlth.anova)

```

#### Stepwise Selection (Tuning Multiple Linear Regression Model)
```{r}
# Perform stepwise selection using AIC as the selection criterion
depression_stepwise_model <- step(depression_model_1, direction = "both", trace = 0, k = log(nrow(train.set)), criterion = "AIC")

mhlth_stepwise_model <- step(mhlth_model_1, direction = "both", trace = 0, k = log(nrow(train.set)), criterion = "AIC")

# Print the final selected model
summary(depression_stepwise_model)
summary(mhlth_stepwise_model)
```

#### Best Model vs. Stepwise Model
```{r}
dep_anova_model_2 <- anova(depression_model_1, depression_stepwise_model)
mhlth_anova_model_2 <- anova(mhlth_model_1, mhlth_stepwise_model)

xkabledply(dep_anova_model_2)
xkabledply(mhlth_anova_model_2)
```

#### Best Model Performance
```{r}
dep_predictions <- predict(depression_model_1, newdata = test.set)

# calculate the R-squared value
r_squared <- summary(depression_model_1)$r.squared

# calculate the mean squared error (MSE)
mse <- mean((test.set$DEPRESSION - dep_predictions)^2)

# print the results
cat("R-squared:", r_squared, "\n")
cat("MSE:", mse, "\n")

mhlth_predictions <- predict(mhlth_stepwise_model, newdata = test.set)

# calculate the R-squared value
r_squared <- summary(mhlth_stepwise_model)$r.squared

# calculate the mean squared error (MSE)
mse <- mean((test.set$MHLTH - mhlth_predictions)^2)

# print the results
cat("R-squared:", r_squared, "\n")
cat("MSE:", mse, "\n")
```


### Regression Tree Model
- Splitting the data: The dataset is split into training and testing sets to evaluate the model's performance.
- Building the tree: The regression tree is built using a recursive partitioning algorithm, which splits the data into subsets based on the independent variables that best predict the dependent variable.
- Pruning the tree: The tree is pruned to reduce overfitting and improve generalization to new data.
```{r}
#regression tree model for depression
# control_params1 <- rpart.control(minsplit = 20, cp = 0.0005, maxdepth = 10)
# 
# depression.tree <- rpart(DEPRESSION ~ LPA + ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + URBANIZED_AREA_POP_CEN_2010 + pct_URBANIZED_AREA_POP_CEN_2010, data = depression_cor_df, method="class", control = control_params1)
# 
# plot(depression.tree, uniform=TRUE, main="Depression Rate Tree")
# text(depression.tree, use.n=TRUE, all=TRUE, cex=.8)
# 
# p <- prp(depression.tree)
# p$nodepar$lab.cex <- 0.8
# p$nodepar$cex <- 0.8
# p$nodepar$lab.col <- "black"
# 
# 
# # regression tree model for poor mental health
# control_params2 <- rpart.control(minsplit = 20, cp = 0.0005, maxdepth = 10)
# 
# mhlth.tree <- rpart(MHLTH ~ LPA + SLEEP + mt.nev.mar + mt.now.mar + MT_Separated + ea.less.hs.deg + ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed, data = mhlth_cor_df, method = "class", control = control_params2)
# 
# plot(mhlth.tree, uniform=TRUE, main="Poor Mental Health Rate Tree")
# text(mhlth.tree, use.n=TRUE, all=TRUE, cex=.8)
# 
# p <- prp(mhlth.tree)
# p$nodepar$lab.cex <- 0.8
# p$nodepar$cex <- 0.8
# p$nodepar$lab.col <- "black"
  

# fancy tree plots
# fancy depression tree
```

```{r}

depression.tree <- rpart(DEPRESSION ~  ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = new_num_df, control = rpart.control(maxdepth = ))

fancyRpartPlot(depression.tree, main = "Depression Rate Tree")

summary(depression.tree)

# fancy mental health tree
mhlth.tree <- rpart(MHLTH ~ ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = new_num_df, control = rpart.control(maxdepth = ))


fancyRpartPlot(mhlth.tree, main = "Poor Mental Health Rate Tree")

summary(mhlth.tree)

# pruning trees
# pruned depression tree
# pruned.depression.tree.2 <- prune(depression.tree.2, cp = 0.01)
# fancyRpartPlot(pruned.depression.tree.2)
# 
# # pruned mental health tree
# pruned.mhlth.tree.2 <- prune(mhlth.tree.2, cp = 0.01)
# fancyRpartPlot(pruned.mhlth.tree.2)

```

```{r}
# creating trees using testing and training data
train.dep.tree <- rpart(DEPRESSION ~  ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set, control = rpart.control(maxdepth = ))

fancyRpartPlot(train.dep.tree, main = "Training Depression Rate Tree")
summary(train.dep.tree)

train.mhlth.tree <- rpart(MHLTH ~ ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set, control = rpart.control(maxdepth=4))

fancyRpartPlot(train.mhlth.tree, main = "Poor Mental Health Rate Tree")

summary(train.mhlth.tree)
```

#### Regression Tree Performance
- Evaluating the model: The model is evaluated using various metrics such as mean squared error (MSE), root mean squared error (RMSE), and R-squared.
- Validating the model: The model is validated using the testing set to assess its ability to generalize to new data.
- Tuning the model: The model is tuned by adjusting the hyperparameters to improve its performance.
- Interpreting the model: The regression tree is interpreted to gain insights into the relationships between the independent variables and the dependent variable.
```{r}
# Generate predicted values for test set
test.set$pred.depression <- predict(train.dep.tree, newdata = test.set)

# Calculate evaluation metrics
MSE <- mean((test.set$DEPRESSION - test.set$pred.depression)^2)
R2 <- 1 - sum((test.set$DEPRESSION - test.set$pred.depression)^2) / sum((test.set$DEPRESSION - mean(test.set$DEPRESSION))^2)

# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")

# The R-squared value of 0.399 suggests that the regression tree model explains 39.9% of the variation in the data, which is a moderate level of explanation.
# The MSE (Mean Squared Error) of 6.05 represents the average squared difference between the predicted values and the actual values, with a higher value indicating worse performance.

test.set$pred.mhlth <- predict(train.mhlth.tree, newdata = test.set)
# Calculate evaluation metrics
MSE <- mean((test.set$MHLTH - test.set$pred.mhlth)^2)
R2 <- 1 - sum((test.set$MHLTH - test.set$pred.mhlth)^2) / sum((test.set$MHLTH - mean(test.set$MHLTH))^2)

# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")

# The regression tree model with MHLTH as the target variable has an R-squared value of 0.685, indicating that 68.5% of the variability in the MHLTH variable can be explained by the model. 
# The mean squared error (MSE) is 1.07, which means that on average, the model's predictions are off by 1.07 units from the actual values. .
# The root mean squared error (RMSE) is 1.04, which is the same as the standard deviation of the residuals.
# This indicates that the model's predictions have a standard deviation of 1.04 around the actual values. Overall, these metrics suggest that the model has a moderate level of predictive power for the MHLTH variable.
```

## Result
Need edited

## Discussion
Need edited

### Limitations
Need edited

### Further Research
Need edited

## Conclusion
Need edited

## References
Need edited
# Reference links to datasets:
https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-Census-Tract-D/cwsq-ngmh
https://gisgeography.com/us-county-map/
https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data/
https://data.census.gov/
https://www.anl.gov/dis/county-economic-impact-index
https://www.census.gov/programs-surveys/community-resilience-estimates/data/datasets.html
https://github.com/NREL/hsds-examples
