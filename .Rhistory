cat("MSE:", mse, "\n")
#regression tree model for depression
# control_params1 <- rpart.control(minsplit = 20, cp = 0.0005, maxdepth = 10)
#
# depression.tree <- rpart(DEPRESSION ~ LPA + ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + URBANIZED_AREA_POP_CEN_2010 + pct_URBANIZED_AREA_POP_CEN_2010, data = depression_cor_df, method="class", control = control_params1)
#
# plot(depression.tree, uniform=TRUE, main="Depression Rate Tree")
# text(depression.tree, use.n=TRUE, all=TRUE, cex=.8)
#
# p <- prp(depression.tree)
# p$nodepar$lab.cex <- 0.8
# p$nodepar$cex <- 0.8
# p$nodepar$lab.col <- "black"
#
#
# # regression tree model for poor mental health
# control_params2 <- rpart.control(minsplit = 20, cp = 0.0005, maxdepth = 10)
#
# mhlth.tree <- rpart(MHLTH ~ LPA + SLEEP + mt.nev.mar + mt.now.mar + MT_Separated + ea.less.hs.deg + ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed, data = mhlth_cor_df, method = "class", control = control_params2)
#
# plot(mhlth.tree, uniform=TRUE, main="Poor Mental Health Rate Tree")
# text(mhlth.tree, use.n=TRUE, all=TRUE, cex=.8)
#
# p <- prp(mhlth.tree)
# p$nodepar$lab.cex <- 0.8
# p$nodepar$cex <- 0.8
# p$nodepar$lab.col <- "black"
# fancy tree plots
# fancy depression tree
depression.tree <- rpart(DEPRESSION ~  ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = new_num_df, control = rpart.control(maxdepth = ))
fancyRpartPlot(depression.tree, main = "Depression Rate Tree")
summary(depression.tree)
# fancy mental health tree
mhlth.tree <- rpart(MHLTH ~ ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = new_num_df, control = rpart.control(maxdepth = ))
fancyRpartPlot(mhlth.tree, main = "Poor Mental Health Rate Tree")
summary(mhlth.tree)
# pruning trees
# pruned depression tree
# pruned.depression.tree.2 <- prune(depression.tree.2, cp = 0.01)
# fancyRpartPlot(pruned.depression.tree.2)
#
# # pruned mental health tree
# pruned.mhlth.tree.2 <- prune(mhlth.tree.2, cp = 0.01)
# fancyRpartPlot(pruned.mhlth.tree.2)
# creating trees using testing and training data
train.dep.tree <- rpart(DEPRESSION ~  ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set, control = rpart.control(maxdepth = ))
fancyRpartPlot(train.dep.tree, main = "Training Depression Rate Tree")
summary(train.dep.tree)
train.mhlth.tree <- rpart(MHLTH ~ ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set, control = rpart.control(maxdepth=4))
fancyRpartPlot(train.mhlth.tree, main = "Poor Mental Health Rate Tree")
summary(train.mhlth.tree)
# Generate predicted values for test set
test.set$pred.depression <- predict(train.dep.tree, newdata = test.set)
# Calculate evaluation metrics
MSE <- mean((test.set$DEPRESSION - test.set$pred.depression)^2)
R2 <- 1 - sum((test.set$DEPRESSION - test.set$pred.depression)^2) / sum((test.set$DEPRESSION - mean(test.set$DEPRESSION))^2)
# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")
# The R-squared value of 0.399 suggests that the regression tree model explains 39.9% of the variation in the data, which is a moderate level of explanation.
# The MSE (Mean Squared Error) of 6.05 represents the average squared difference between the predicted values and the actual values, with a higher value indicating worse performance.
test.set$pred.mhlth <- predict(train.mhlth.tree, newdata = test.set)
# Calculate evaluation metrics
MSE <- mean((test.set$MHLTH - test.set$pred.mhlth)^2)
R2 <- 1 - sum((test.set$MHLTH - test.set$pred.mhlth)^2) / sum((test.set$MHLTH - mean(test.set$MHLTH))^2)
# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")
# The regression tree model with MHLTH as the target variable has an R-squared value of 0.685, indicating that 68.5% of the variability in the MHLTH variable can be explained by the model.
# The mean squared error (MSE) is 1.07, which means that on average, the model's predictions are off by 1.07 units from the actual values. .
# The root mean squared error (RMSE) is 1.04, which is the same as the standard deviation of the residuals.
# This indicates that the model's predictions have a standard deviation of 1.04 around the actual values. Overall, these metrics suggest that the model has a moderate level of predictive power for the MHLTH variable.
# Some of common RMD options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
rm(list=ls())
library(readr)
library(ggplot2)
library(ggpubr)
library(tidyr)
library(corrplot)
library(ezids)
library(car)
library(rpart)
library(rpart.plot)
library(rattle)
library(tree)
url <- 'https://raw.githubusercontent.com/eitanaka/DATS6101_Final_Project_Team2/main/data_set/geo_socio_health_df.csv'
master_df<- read_csv(url)
master_df <- master_df[,-1]
# To rename some columns in a data frame to make them more readable and easier to work with.This is done by using the "colnames" function to first select the column names that match the original names, and then assigning new names.
colnames(master_df)[colnames(master_df) == "MT_Never Married"] <- "mt.nev.mar"
colnames(master_df)[colnames(master_df) == "MT_Now married"] <- "mt.now.mar"
colnames(master_df)[colnames(master_df) == "Total Population"] <- "tot.pop"
colnames(master_df)[colnames(master_df) == "EA_Less than high school graduate"] <- "ea.less.hs.deg"
colnames(master_df)[colnames(master_df) == "EA_High school graduate"] <- "ea.hs.deg"
colnames(master_df)[colnames(master_df) == "EA_college or associate's degree"] <- "ea.col.ass.deg"
colnames(master_df)[colnames(master_df) == "EA_Bachelor's degree"] <- "ea.ba.deg"
colnames(master_df)[colnames(master_df) == "EA_Graduate or professional degree"] <- "ea.grad.prof.deg"
# Create a new data only including numerical variable
numeric_vars <- sapply(master_df, is.numeric)
num_df <- master_df[, numeric_vars]
str(num_df)
summary(num_df)
colSums(is.na(num_df))
num_df <- na.omit(num_df)
new_num_df <- outlierKD2(num_df, num_df[[1]], rm=TRUE, boxplt=F, histogram=F,qqplt=F)
new_num_df <- new_num_df[,1:(ncol(new_num_df)-2)]
# loop through all columns
for (col_name in colnames(num_df)[2:ncol(num_df)]) {
# remove outliers
new_num_df <- outlierKD2(new_num_df, new_num_df[[col_name]], rm=T, boxplt=F, histogram=F,qqplt=F)
new_num_df <- na.omit(new_num_df)
new_num_df <- new_num_df[,1:(ncol(new_num_df)-2)]
}
new_num_df <- new_num_df[,1:(ncol(new_num_df)-1)]
par(mfrow=c(8, 8))
par(mar=c(1, 1, 1, 1))
for (i in 1:ncol(new_num_df)) {
boxplot(new_num_df[[i]], main=names(new_num_df)[i])
}
theme_set(theme_pubr(base_size = 3.5))
plot_list <- list()
for (col in names(new_num_df)) {
if (col != "DEPRESSION") {
plot_data <- data.frame(x = new_num_df[[col]], y = new_num_df$DEPRESSION)
plot <- ggplot(plot_data, aes(x = x, y = y)) +
geom_point(size=0.3) +
geom_smooth(method = "lm", se = FALSE) +
xlab(col) +
theme(legend.position = "none")
plot_list[[col]] <- plot
}
}
ggarrange(plotlist = plot_list, ncol = 7, nrow = 8)
# Scatter plots with fit line for mhlth and other dependent variables
theme_set(theme_pubr(base_size = 3.5))
plot_list <- list()
for (col in names(new_num_df)) {
if (col != "MHLTH") {
plot_data <- data.frame(x = new_num_df[[col]], y = new_num_df$MHLTH)
plot <- ggplot(plot_data, aes(x = x, y = y)) +
geom_point(size=0.3) +
geom_smooth(method = "lm", se = FALSE) +
xlab(col) +
theme(legend.position = "none")
plot_list[[col]] <- plot
}
}
ggarrange(plotlist = plot_list, ncol = 7, nrow = 8)
# Histograms
hist(new_num_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")
hist(new_num_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")
# QQ plots for the distribution of tract-level depression rates and tract-level poor mental health rates.
qqnorm(new_num_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")
qqline(new_num_df$DEPRESSION)
qqnorm(new_num_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")
qqline(new_num_df$MHLTH)
# Create a correlation matrix
cor_matrix <- cor(new_num_df)
# Create two lists which have the names of variables highly correlated (more then 0.3 or less than -0.3)
high_dep_cor_list <- names(which(cor_matrix["DEPRESSION",] > 0.35 | cor_matrix["DEPRESSION",] < -0.35))
high_mhlth_cor_list <- names(which(cor_matrix["MHLTH",] > 0.5 | cor_matrix["MHLTH",] < -0.5))
high_dep_cor_list <- high_dep_cor_list[high_dep_cor_list != "MHLTH"]
high_mhlth_cor_list <- high_mhlth_cor_list[high_mhlth_cor_list != "DEPRESSION"]
# Create two correlation matrixes from new_num_df in the above lists
high_dep_cor_mat <- cor(new_num_df[high_dep_cor_list])
high_mhlth_cor_mat <- cor(new_num_df[high_mhlth_cor_list])
# Plot above correlation matrix
corrplot(high_dep_cor_mat, type = "lower", outline.color = "white",
colors = c("#6D9EC1", "white", "#E46726"), legend.title = "Correlation",
ggtheme = theme_gray, title = "Highly Correlated Variables with DEPRESSION")
corrplot(high_mhlth_cor_mat, type = "lower", outline.color = "white",
colors = c("#6D9EC1", "white", "#E46726"), legend.title = "Correlation",
ggtheme = theme_gray, title = "Highly Correlated Variables with mhlth")
my_col <- colorRampPalette(c("red", "white", "blue"))(30)
corrplot(high_dep_cor_mat, method = "number", type = "lower", order = "hclust", diag = FALSE, tl.col = "black", tl.cex = 0.8, cl.cex = 0.8, addCoef.col = "black", col = my_col, main = "", mar = c(0,0,0,0))
corrplot(high_mhlth_cor_mat, method = "number", type = "lower", order = "hclust", diag = FALSE, tl.col = "black", tl.cex = 0.8, cl.cex = 0.8, addCoef.col = "black", col = my_col, main = "", mar = c(0,0,0,0))
calculate_VIF <- function(data, target_col) {
X <- data[, !colnames(data) %in% target_col]
vif <- data.frame(
Feature = colnames(X),
VIF = apply(X, 2, function(x) vif(lm(x ~ ., data=X)))
)
return(vif)
}
depression_VIF <- calculate_VIF(new_num_df[high_dep_cor_list], "DEPRESSION")
mhlth_VIF <- calculate_VIF(new_num_df[high_mhlth_cor_list], "MHLTH")
depression_VIF
mhlth_VIF
dep_features_list <- high_dep_cor_list[high_dep_cor_list != "DEPRESSION"]
mhlth_features_list <- high_mhlth_cor_list[high_mhlth_cor_list != "MHLTH"]
dep_features_list
mhlth_features_list
library(caret)
set.seed(123)
fold <- floor(runif(nrow(new_num_df),1,11))
new_num_df$fold <- fold
test.set <- new_num_df[new_num_df$fold == 1,]
train.set <- new_num_df[new_num_df$fold != 1,]
# multiple linear regression model for depression
depression_model_1 <- lm(DEPRESSION ~ OBESITY + PHLTH + ea.hs.deg + ea.grad.prof.deg + MI_Estimate + `CT_>60` + hhd.broad, data = train.set)
# multiple linear regression model for poor mental health
mhlth_model_1 <- lm(MHLTH ~ LPA + OBESITY + PHLTH + SLEEP + ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + poverty, data = train.set)
summary(depression_model_1)
summary(mhlth_model_1)
plot(depression_model_1)
plot(mhlth_model_1)
# multiple linear regression model for depression
depression_model_2 <- lm(DEPRESSION ~ OBESITY + PHLTH, data = train.set)
# multiple linear regression model for poor mental health
mhlth_model_2 <- lm(MHLTH ~ LPA + OBESITY + PHLTH + SLEEP, data = train.set)
summary(depression_model_2)
summary(mhlth_model_2)
plot(depression_model_2)
plot(mhlth_model_2)
# multiple linear regression model for depression
depression_model_3 <- lm(DEPRESSION ~ ea.hs.deg + ea.grad.prof.deg + MI_Estimate + `CT_>60` + hhd.broad, data = train.set)
# multiple linear regression model for poor mental health
mhlth_model_3 <- lm(MHLTH ~ ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + poverty, data = train.set)
summary(depression_model_3)
summary(mhlth_model_3)
plot(depression_model_3)
plot(mhlth_model_3)
# Perform two-way ANOVA test
depression_anova_model <- anova(depression_model_1, depression_model_2, depression_model_3)
mhlth_anova_model <- anova(mhlth_model_1, mhlth_model_2, mhlth_model_3)
xkabledply(depression_anova_model)
xkabledply(mhlth_anova_model)
# health.socioecon.dep.anova <- anova(depression_model_2, depression_model_3)
# health.socioecon.mhlth.anova <- anova(mhlth_model_2, mhlth_model_3)
#
# xkabledply(health.socioecon.dep.anova)
# xkabledply(health.socioecon.mhlth.anova)
# Perform stepwise selection using AIC as the selection criterion
depression_stepwise_model <- step(depression_model_1, direction = "both", trace = 0, k = log(nrow(train.set)), criterion = "AIC")
mhlth_stepwise_model <- step(mhlth_model_1, direction = "both", trace = 0, k = log(nrow(train.set)), criterion = "AIC")
dep_anova_model_2 <- anova(depression_model_1, depression_stepwise_model)
mhlth_anova_model_2 <- anova(mhlth_model_1, mhlth_stepwise_model)
xkabledply(dep_anova_model_2)
xkabledply(mhlth_anova_model_2)
dep_predictions <- predict(depression_model_1, newdata = test.set)
# calculate the R-squared value
r_squared <- summary(depression_model_1)$r.squared
# calculate the mean squared error (MSE)
mse <- mean((test.set$DEPRESSION - dep_predictions)^2)
# print the results
cat("R-squared:", r_squared, "\n")
cat("MSE:", mse, "\n")
mhlth_predictions <- predict(mhlth_stepwise_model, newdata = test.set)
# calculate the R-squared value
r_squared <- summary(mhlth_stepwise_model)$r.squared
# calculate the mean squared error (MSE)
mse <- mean((test.set$MHLTH - mhlth_predictions)^2)
# print the results
cat("R-squared:", r_squared, "\n")
cat("MSE:", mse, "\n")
# Building a Regression Tree
# Step 1. Use recursive binary splitting to grow a large tree of depression on the training data
train.dep.tree <- rpart(DEPRESSION ~  ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set, method = "anova")
# Grow large tree of mhlth on the training data
train.mhlth.tree <- rpart(MHLTH ~ ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set)
# Plot tree model
fancyRpartPlot(train.dep.tree, main = "Depression Rate Tree")
fancyRpartPlot(train.mhlth.tree, main = "Poor Mental Health Rate Tree")
# Print summary
summary(train.dep.tree)
summary(train.mhlth.tree)
# Generate predicted values for test set
test.set$pred.depression <- predict(train.dep.tree, newdata = test.set)
# Calculate evaluation metrics
MSE <- mean((test.set$DEPRESSION - test.set$pred.depression)^2)
R2 <- 1 - sum((test.set$DEPRESSION - test.set$pred.depression)^2) / sum((test.set$DEPRESSION - mean(test.set$DEPRESSION))^2)
# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")
# The R-squared value of 0.399 suggests that the regression tree model explains 39.9% of the variation in the data, which is a moderate level of explanation.
# The MSE (Mean Squared Error) of 6.05 represents the average squared difference between the predicted values and the actual values, with a higher value indicating worse performance.
test.set$pred.mhlth <- predict(train.mhlth.tree, newdata = test.set)
# Calculate evaluation metrics
MSE <- mean((test.set$MHLTH - test.set$pred.mhlth)^2)
R2 <- 1 - sum((test.set$MHLTH - test.set$pred.mhlth)^2) / sum((test.set$MHLTH - mean(test.set$MHLTH))^2)
# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")
# The regression tree model with MHLTH as the target variable has an R-squared value of 0.685, indicating that 68.5% of the variability in the MHLTH variable can be explained by the model.
# The mean squared error (MSE) is 1.07, which means that on average, the model's predictions are off by 1.07 units from the actual values. .
# The root mean squared error (RMSE) is 1.04, which is the same as the standard deviation of the residuals.
# This indicates that the model's predictions have a standard deviation of 1.04 around the actual values. Overall, these metrics suggest that the model has a moderate level of predictive power for the MHLTH variable.
# Some of common RMD options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
rm(list=ls())
library(readr)
library(ggplot2)
library(ggpubr)
library(tidyr)
library(corrplot)
library(ezids)
library(car)
library(rpart)
library(rpart.plot)
library(rattle)
library(tree)
url <- 'https://raw.githubusercontent.com/eitanaka/DATS6101_Final_Project_Team2/main/data_set/geo_socio_health_df.csv'
master_df<- read_csv(url)
master_df <- master_df[,-1]
# To rename some columns in a data frame to make them more readable and easier to work with.This is done by using the "colnames" function to first select the column names that match the original names, and then assigning new names.
colnames(master_df)[colnames(master_df) == "MT_Never Married"] <- "mt.nev.mar"
colnames(master_df)[colnames(master_df) == "MT_Now married"] <- "mt.now.mar"
colnames(master_df)[colnames(master_df) == "Total Population"] <- "tot.pop"
colnames(master_df)[colnames(master_df) == "EA_Less than high school graduate"] <- "ea.less.hs.deg"
colnames(master_df)[colnames(master_df) == "EA_High school graduate"] <- "ea.hs.deg"
colnames(master_df)[colnames(master_df) == "EA_college or associate's degree"] <- "ea.col.ass.deg"
colnames(master_df)[colnames(master_df) == "EA_Bachelor's degree"] <- "ea.ba.deg"
colnames(master_df)[colnames(master_df) == "EA_Graduate or professional degree"] <- "ea.grad.prof.deg"
# Create a new data only including numerical variable
numeric_vars <- sapply(master_df, is.numeric)
num_df <- master_df[, numeric_vars]
str(num_df)
summary(num_df)
colSums(is.na(num_df))
num_df <- na.omit(num_df)
new_num_df <- outlierKD2(num_df, num_df[[1]], rm=TRUE, boxplt=F, histogram=F,qqplt=F)
new_num_df <- new_num_df[,1:(ncol(new_num_df)-2)]
# loop through all columns
for (col_name in colnames(num_df)[2:ncol(num_df)]) {
# remove outliers
new_num_df <- outlierKD2(new_num_df, new_num_df[[col_name]], rm=T, boxplt=F, histogram=F,qqplt=F)
new_num_df <- na.omit(new_num_df)
new_num_df <- new_num_df[,1:(ncol(new_num_df)-2)]
}
new_num_df <- new_num_df[,1:(ncol(new_num_df)-1)]
par(mfrow=c(8, 8))
par(mar=c(1, 1, 1, 1))
for (i in 1:ncol(new_num_df)) {
boxplot(new_num_df[[i]], main=names(new_num_df)[i])
}
theme_set(theme_pubr(base_size = 3.5))
plot_list <- list()
for (col in names(new_num_df)) {
if (col != "DEPRESSION") {
plot_data <- data.frame(x = new_num_df[[col]], y = new_num_df$DEPRESSION)
plot <- ggplot(plot_data, aes(x = x, y = y)) +
geom_point(size=0.3) +
geom_smooth(method = "lm", se = FALSE) +
xlab(col) +
theme(legend.position = "none")
plot_list[[col]] <- plot
}
}
ggarrange(plotlist = plot_list, ncol = 7, nrow = 8)
# Scatter plots with fit line for mhlth and other dependent variables
theme_set(theme_pubr(base_size = 3.5))
plot_list <- list()
for (col in names(new_num_df)) {
if (col != "MHLTH") {
plot_data <- data.frame(x = new_num_df[[col]], y = new_num_df$MHLTH)
plot <- ggplot(plot_data, aes(x = x, y = y)) +
geom_point(size=0.3) +
geom_smooth(method = "lm", se = FALSE) +
xlab(col) +
theme(legend.position = "none")
plot_list[[col]] <- plot
}
}
ggarrange(plotlist = plot_list, ncol = 7, nrow = 8)
# Histograms
hist(new_num_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")
hist(new_num_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")
# QQ plots for the distribution of tract-level depression rates and tract-level poor mental health rates.
qqnorm(new_num_df$DEPRESSION, main = "Distribution of Tract-level Depression Rates")
qqline(new_num_df$DEPRESSION)
qqnorm(new_num_df$MHLTH, main = "Distribution of Tract-level Poor Mental Health Rates")
qqline(new_num_df$MHLTH)
# Create a correlation matrix
cor_matrix <- cor(new_num_df)
# Create two lists which have the names of variables highly correlated (more then 0.3 or less than -0.3)
high_dep_cor_list <- names(which(cor_matrix["DEPRESSION",] > 0.35 | cor_matrix["DEPRESSION",] < -0.35))
high_mhlth_cor_list <- names(which(cor_matrix["MHLTH",] > 0.5 | cor_matrix["MHLTH",] < -0.5))
high_dep_cor_list <- high_dep_cor_list[high_dep_cor_list != "MHLTH"]
high_mhlth_cor_list <- high_mhlth_cor_list[high_mhlth_cor_list != "DEPRESSION"]
# Create two correlation matrixes from new_num_df in the above lists
high_dep_cor_mat <- cor(new_num_df[high_dep_cor_list])
high_mhlth_cor_mat <- cor(new_num_df[high_mhlth_cor_list])
# Plot above correlation matrix
corrplot(high_dep_cor_mat, type = "lower", outline.color = "white",
colors = c("#6D9EC1", "white", "#E46726"), legend.title = "Correlation",
ggtheme = theme_gray, title = "Highly Correlated Variables with DEPRESSION")
corrplot(high_mhlth_cor_mat, type = "lower", outline.color = "white",
colors = c("#6D9EC1", "white", "#E46726"), legend.title = "Correlation",
ggtheme = theme_gray, title = "Highly Correlated Variables with mhlth")
my_col <- colorRampPalette(c("red", "white", "blue"))(30)
corrplot(high_dep_cor_mat, method = "number", type = "lower", order = "hclust", diag = FALSE, tl.col = "black", tl.cex = 0.8, cl.cex = 0.8, addCoef.col = "black", col = my_col, main = "", mar = c(0,0,0,0))
corrplot(high_mhlth_cor_mat, method = "number", type = "lower", order = "hclust", diag = FALSE, tl.col = "black", tl.cex = 0.8, cl.cex = 0.8, addCoef.col = "black", col = my_col, main = "", mar = c(0,0,0,0))
calculate_VIF <- function(data, target_col) {
X <- data[, !colnames(data) %in% target_col]
vif <- data.frame(
Feature = colnames(X),
VIF = apply(X, 2, function(x) vif(lm(x ~ ., data=X)))
)
return(vif)
}
depression_VIF <- calculate_VIF(new_num_df[high_dep_cor_list], "DEPRESSION")
mhlth_VIF <- calculate_VIF(new_num_df[high_mhlth_cor_list], "MHLTH")
depression_VIF
mhlth_VIF
dep_features_list <- high_dep_cor_list[high_dep_cor_list != "DEPRESSION"]
mhlth_features_list <- high_mhlth_cor_list[high_mhlth_cor_list != "MHLTH"]
dep_features_list
mhlth_features_list
library(caret)
set.seed(123)
fold <- floor(runif(nrow(new_num_df),1,11))
new_num_df$fold <- fold
test.set <- new_num_df[new_num_df$fold == 1,]
train.set <- new_num_df[new_num_df$fold != 1,]
# multiple linear regression model for depression
depression_model_1 <- lm(DEPRESSION ~ OBESITY + PHLTH + ea.hs.deg + ea.grad.prof.deg + MI_Estimate + `CT_>60` + hhd.broad, data = train.set)
# multiple linear regression model for poor mental health
mhlth_model_1 <- lm(MHLTH ~ LPA + OBESITY + PHLTH + SLEEP + ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + poverty, data = train.set)
summary(depression_model_1)
summary(mhlth_model_1)
plot(depression_model_1)
plot(mhlth_model_1)
# multiple linear regression model for depression
depression_model_2 <- lm(DEPRESSION ~ OBESITY + PHLTH, data = train.set)
# multiple linear regression model for poor mental health
mhlth_model_2 <- lm(MHLTH ~ LPA + OBESITY + PHLTH + SLEEP, data = train.set)
summary(depression_model_2)
summary(mhlth_model_2)
plot(depression_model_2)
plot(mhlth_model_2)
# multiple linear regression model for depression
depression_model_3 <- lm(DEPRESSION ~ ea.hs.deg + ea.grad.prof.deg + MI_Estimate + `CT_>60` + hhd.broad, data = train.set)
# multiple linear regression model for poor mental health
mhlth_model_3 <- lm(MHLTH ~ ea.hs.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + poverty, data = train.set)
summary(depression_model_3)
summary(mhlth_model_3)
plot(depression_model_3)
plot(mhlth_model_3)
# Perform two-way ANOVA test
depression_anova_model <- anova(depression_model_1, depression_model_2, depression_model_3)
mhlth_anova_model <- anova(mhlth_model_1, mhlth_model_2, mhlth_model_3)
xkabledply(depression_anova_model)
xkabledply(mhlth_anova_model)
# health.socioecon.dep.anova <- anova(depression_model_2, depression_model_3)
# health.socioecon.mhlth.anova <- anova(mhlth_model_2, mhlth_model_3)
#
# xkabledply(health.socioecon.dep.anova)
# xkabledply(health.socioecon.mhlth.anova)
# Perform stepwise selection using AIC as the selection criterion
depression_stepwise_model <- step(depression_model_1, direction = "both", trace = 0, k = log(nrow(train.set)), criterion = "AIC")
mhlth_stepwise_model <- step(mhlth_model_1, direction = "both", trace = 0, k = log(nrow(train.set)), criterion = "AIC")
dep_anova_model_2 <- anova(depression_model_1, depression_stepwise_model)
mhlth_anova_model_2 <- anova(mhlth_model_1, mhlth_stepwise_model)
xkabledply(dep_anova_model_2)
xkabledply(mhlth_anova_model_2)
dep_predictions <- predict(depression_model_1, newdata = test.set)
# calculate the R-squared value
r_squared <- summary(depression_model_1)$r.squared
# calculate the mean squared error (MSE)
mse <- mean((test.set$DEPRESSION - dep_predictions)^2)
# print the results
cat("R-squared:", r_squared, "\n")
cat("MSE:", mse, "\n")
mhlth_predictions <- predict(mhlth_stepwise_model, newdata = test.set)
# calculate the R-squared value
r_squared <- summary(mhlth_stepwise_model)$r.squared
# calculate the mean squared error (MSE)
mse <- mean((test.set$MHLTH - mhlth_predictions)^2)
# print the results
cat("R-squared:", r_squared, "\n")
cat("MSE:", mse, "\n")
# Building a Regression Tree
# Step 1. Use recursive binary splitting to grow a large tree of depression on the training data
train.dep.tree <- rpart(DEPRESSION ~  ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set, method = "anova")
# Grow large tree of mhlth on the training data
train.mhlth.tree <- rpart(MHLTH ~ ACCESS2 + BINGE + CHECKUP + DIABETES + LPA + OBESITY + PHLTH + SLEEP + STROKE + mt.nev.mar + mt.now.mar + MT_Divorces + MT_Separated + MT_Widowed + ea.less.hs.deg + ea.hs.deg + ea.col.ass.deg + ea.ba.deg + ea.grad.prof.deg + MI_Estimate + tot.pop + `CT_<10` + `CT_10-14` + `CT_15-19` + `CT_20-24` + `CT_25-29` + `CT_30-34` + `CT_35-44` + `CT_45-59` + `CT_>60` + ES_Total_labor_force + ES_Civilian_labor_force + ES_Civilian_labor_force_employed + ES_Civilian_labor_force_unemployed + ES_Armed_Forces + ES_Not_in_labor_force + land + urban + poverty + no.ins + disab + no.comp + `broad&comp` + no.eng + sing.mom + live.alone + pub.assist + no.phone + no.plumb + married.kid + hhd.no.comp + hhd.only.phone + hhd.no.int + hhd.broad + index_apr20, data = train.set)
# Plot tree model
fancyRpartPlot(train.dep.tree, main = "Depression Rate Tree")
fancyRpartPlot(train.mhlth.tree, main = "Poor Mental Health Rate Tree")
# Print summary
summary(train.dep.tree)
summary(train.mhlth.tree)
# Generate predicted values for test set
test.set$pred.depression <- predict(train.dep.tree, newdata = test.set)
# Calculate evaluation metrics
MSE <- mean((test.set$DEPRESSION - test.set$pred.depression)^2)
R2 <- 1 - sum((test.set$DEPRESSION - test.set$pred.depression)^2) / sum((test.set$DEPRESSION - mean(test.set$DEPRESSION))^2)
# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")
# The R-squared value of 0.399 suggests that the regression tree model explains 39.9% of the variation in the data, which is a moderate level of explanation.
# The MSE (Mean Squared Error) of 6.05 represents the average squared difference between the predicted values and the actual values, with a higher value indicating worse performance.
test.set$pred.mhlth <- predict(train.mhlth.tree, newdata = test.set)
# Calculate evaluation metrics
MSE <- mean((test.set$MHLTH - test.set$pred.mhlth)^2)
R2 <- 1 - sum((test.set$MHLTH - test.set$pred.mhlth)^2) / sum((test.set$MHLTH - mean(test.set$MHLTH))^2)
# Print evaluation metrics
cat("R-squared:", R2, "\n")
cat("MSE:", MSE, "\n")
# The regression tree model with MHLTH as the target variable has an R-squared value of 0.685, indicating that 68.5% of the variability in the MHLTH variable can be explained by the model.
# The mean squared error (MSE) is 1.07, which means that on average, the model's predictions are off by 1.07 units from the actual values. .
# The root mean squared error (RMSE) is 1.04, which is the same as the standard deviation of the residuals.
# This indicates that the model's predictions have a standard deviation of 1.04 around the actual values. Overall, these metrics suggest that the model has a moderate level of predictive power for the MHLTH variable.
